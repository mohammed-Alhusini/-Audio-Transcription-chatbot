{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "dotnet_interactive": {
     "language": "csharp"
    },
    "polyglot_notebook": {
     "kernelName": "csharp"
    }
   },
   "outputs": [],
   "source": [
    "import os\n",
    "from dotenv import load_dotenv, dotenv_values \n",
    "import IPython.display as ipd\n",
    "from glob import glob\n",
    "from pydub import AudioSegment\n",
    "import wave\n",
    "import contextlib\n",
    "\n",
    "OPENAI_API_KEY = os.getenv('OPENAI_API_KEY')\n",
    "import openai\n",
    "from openai import OpenAI\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "import gradio as gr"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# data handling "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset = glob('../audio_AI/audio_resources/*.wav')\n",
    "sample = dataset[5]\n",
    "ipd.Audio(sample)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_audio_length(file_path):\n",
    "    audio = AudioSegment.from_mp3(file_path)\n",
    "    return len(audio) / (60 * 1000)  # length in minutes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "def split_audio_into_chunks(file_path, chunk_length=10):\n",
    "    if get_audio_length(file_path) > 10 :\n",
    "        \n",
    "\n",
    "        song = AudioSegment.from_mp3(file_path)\n",
    "        total_length = get_audio_length(file_path)  # in minutes\n",
    "        ten_minutes = chunk_length * 60 * 1000  # chunk length in milliseconds\n",
    "\n",
    "        num_chunks = int(total_length // chunk_length) + (1 if total_length % chunk_length > 0 else 0)\n",
    "\n",
    "        for i in range(num_chunks):\n",
    "            start_time = i * ten_minutes\n",
    "            end_time = start_time + ten_minutes if (start_time + ten_minutes) < len(song) else len(song)\n",
    "\n",
    "            chunk = song[start_time:end_time]\n",
    "            chunk.export(f'audio_resources/chunk/audio_chunk_{i + 1}.mp3', format=\"mp3\")\n",
    "    else:\n",
    "        return file_path\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "split_audio_into_chunks('/Users/floky/Desktop/summer traning/projects/audio_AI/audio_resources/videoplayback.wav')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset_chunk = glob('../audio_AI/audio_resources/chunk/*.mp3')\n",
    "\n",
    "sample = dataset_chunk[0]\n",
    "ipd.Audio(sample)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def get_transcription_audio(file_path, model=\"whisper-1\"):\n",
    "    client = OpenAI(api_key =OPENAI_API_KEY)\n",
    "     \n",
    "    audio_file= open(file_path, \"rb\")\n",
    "    transcription = client.audio.transcriptions.create(\n",
    "        model=model,\n",
    "        file=audio_file,\n",
    "        response_format=\"text\"\n",
    "    )\n",
    "\n",
    "    \n",
    "    return transcription\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [],
   "source": [
    "related_context = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in range(len(dataset_chunk)):\n",
    "    related_context.append(get_transcription_audio(dataset_chunk[i]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "related_context"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# prompt engineering PE "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_answer(user_prompt, query):\n",
    "    client = OpenAI(api_key =OPENAI_API_KEY)\n",
    "\n",
    "    completion = client.chat.completions.create(\n",
    "        model=\"gpt-4o\",\n",
    "        messages=[\n",
    "        {\"role\": \"system\", \"content\": user_prompt},\n",
    "        {\"role\": \"user\", \"content\": query},\n",
    "        ],\n",
    "    )\n",
    "    return completion.choices[0].message.content"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "user_prompt = f\"\"\"\n",
    "You are AudioTranscriberBot, an advanced AI language model designed to assist users by transcribing audio files and providing accurate, helpful, and engaging responses about the transcribed content. Your primary role is to:\n",
    "\n",
    "1. Accurately transcribe the uploaded MP3 file.\n",
    "2. Allow users to ask questions about the transcribed content and provide precise and relevant answers.\n",
    "3. Maintain a polite, respectful, and professional tone at all times.\n",
    "4. Provide clear, concise, and accurate information.\n",
    "5. Offer detailed explanations and context when necessary to ensure understanding.\n",
    "6. Show to the user only the answer.\n",
    "\n",
    "NOTE: related_context will be delimited by triple backticks \n",
    "related_context: ```{related_context}```\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "---\n",
    "Your task is to answer in a consistent style.\n",
    "\n",
    "<user>: Can you summarize the key points from the audio?\n",
    "\n",
    "<AudioTranscriberBot>: The key points from the audio are as follows:\n",
    "\n",
    "1. [First key point]\n",
    "2. [Second key point]\n",
    "3. [Third key point]\n",
    "\n",
    "If you need more detailed information on a specific section, please let me know.\n",
    "\"\"\"\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# user interface UI"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## version 1\n",
    "Can only handle previous audio"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "message_list = []\n",
    "response_list = []\n",
    "\n",
    "def AudioTranscriberBot(message, history):\n",
    "    user_prompt = f\"\"\"\n",
    "    You are AudioTranscriberBot, an advanced AI language model designed to assist users by transcribing audio files and providing accurate, helpful, and engaging responses about the transcribed content. Your primary role is to:\n",
    "\n",
    "1. Accurately transcribe the uploaded file.\n",
    "2. Allow users to ask questions about the transcribed content and provide precise and relevant answers.\n",
    "3. Maintain a polite, respectful, and professional tone at all times.\n",
    "4. Provide clear, concise, and accurate information.\n",
    "5. Offer detailed explanations and context when necessary to ensure understanding.\n",
    "6. Show to the user only the answer.\n",
    "\n",
    "NOTE: related_context will be delimited by triple backticks \n",
    "related_context: ```{related_context}```\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "---\n",
    "Your task is to answer in a consistent style.\n",
    "\n",
    "<user>: Can you summarize the key points from the audio?\n",
    "\n",
    "<AudioTranscriberBot>: The key points from the audio are as follows:\n",
    "\n",
    "1. [First key point]\n",
    "2. [Second key point]\n",
    "3. [Third key point]\n",
    "\n",
    "If you need more detailed information on a specific section, please let me know.\n",
    "\"\"\"\n",
    "\n",
    "    conversation = get_answer(user_prompt,message)\n",
    "    return conversation\n",
    "\n",
    "demo_chatbot = gr.ChatInterface(\n",
    "    AudioTranscriberBot,\n",
    "    title=\"Audio Transcriber Bot\",\n",
    "    description=\" Talk to your audio file\",\n",
    "    multimodal=True,\n",
    "\n",
    ")\n",
    "\n",
    "\n",
    "demo_chatbot.launch()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "metadata": {},
   "outputs": [],
   "source": [
    "related_context = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[]"
      ]
     },
     "execution_count": 93,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "related_context"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# version 2 \n",
    "\n",
    "Can handle user uploaded audio"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_transcription_audio(messages,file_path):\n",
    "    model=\"whisper-1\"\n",
    "    client = OpenAI(api_key =OPENAI_API_KEY)\n",
    "    if related_context == []:\n",
    "        if not file_path:\n",
    "            return \"Please upload an audio file to get started.\"\n",
    "        \n",
    "     \n",
    "        audio_file= open(file_path, \"rb\")\n",
    "        transcription = client.audio.transcriptions.create(\n",
    "            model=model,\n",
    "            file=audio_file,\n",
    "            response_format=\"text\"\n",
    "        )\n",
    "\n",
    "        related_context.append(transcription)\n",
    "        \n",
    "        user_prompt = f\"\"\"\n",
    "You are AudioTranscriberBot, an advanced AI language model designed to assist users by transcribing audio files and providing accurate, helpful, and engaging responses about the transcribed content. Your primary role is to:\n",
    "\n",
    "\n",
    "1. Accurately transcribe the uploaded file.\n",
    "2. Allow users to ask questions about the transcribed content and provide precise and relevant answers.\n",
    "3. Maintain a polite, respectful, and professional tone at all times.\n",
    "4. Provide clear, concise, and accurate information.\n",
    "5. Offer detailed explanations and context when necessary to ensure understanding.\n",
    "6. Show to the user only the answer.\n",
    "\n",
    "NOTE: related_context will be delimited by triple backticks \n",
    "related_context: ```{related_context}```\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "---\n",
    "Your task is to answer in a consistent style.\n",
    "\n",
    "<user>: Can you summarize the key points from the audio?\n",
    "\n",
    "<AudioTranscriberBot>: The key points from the audio are as follows:\n",
    "\n",
    "1. [First key point]\n",
    "2. [Second key point]\n",
    "3. [Third key point]\n",
    "\n",
    "If you need more detailed information on a specific section, please let me know.\n",
    "\"\"\"\n",
    "\n",
    "        completion = client.chat.completions.create(\n",
    "            model=\"gpt-4o\",\n",
    "            messages=[\n",
    "            {\"role\": \"system\", \"content\": user_prompt},\n",
    "            {\"role\": \"user\", \"content\": messages},\n",
    "            ],\n",
    "        )\n",
    "        return completion.choices[0].message.content\n",
    "    else:\n",
    "        user_prompt = f\"\"\"\n",
    "You are AudioTranscriberBot, an advanced AI language model designed to assist users by transcribing audio files and providing accurate, helpful, and engaging responses about the transcribed content. Your primary role is to:\n",
    "\n",
    "\n",
    "1. Accurately transcribe the uploaded file.\n",
    "2. Allow users to ask questions about the transcribed content and provide precise and relevant answers.\n",
    "3. Maintain a polite, respectful, and professional tone at all times.\n",
    "4. Provide clear, concise, and accurate information.\n",
    "5. Offer detailed explanations and context when necessary to ensure understanding.\n",
    "6. Show to the user only the answer.\n",
    "\n",
    "NOTE: related_context will be delimited by triple backticks \n",
    "related_context: ```{related_context}```\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "---\n",
    "Your task is to answer in a consistent style.\n",
    "\n",
    "<user>: Can you summarize the key points from the audio?\n",
    "\n",
    "<AudioTranscriberBot>: The key points from the audio are as follows:\n",
    "\n",
    "1. [First key point]\n",
    "2. [Second key point]\n",
    "3. [Third key point]\n",
    "\n",
    "If you need more detailed information on a specific section, please let me know.\n",
    "\"\"\"\n",
    "        completion = client.chat.completions.create(\n",
    "            model=\"gpt-4o\",\n",
    "            messages=[\n",
    "            {\"role\": \"system\", \"content\": user_prompt},\n",
    "            {\"role\": \"user\", \"content\": messages},\n",
    "            ],\n",
    "        )\n",
    "        return completion.choices[0].message.content"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Running on local URL:  http://127.0.0.1:7897\n",
      "\n",
      "To create a public link, set `share=True` in `launch()`.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div><iframe src=\"http://127.0.0.1:7897/\" width=\"100%\" height=\"500\" allow=\"autoplay; camera; microphone; clipboard-read; clipboard-write;\" frameborder=\"0\" allowfullscreen></iframe></div>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "interface_test = gr.Interface(\n",
    "    fn=get_transcription_audio,\n",
    "    inputs=['text', 'file'],\n",
    "    outputs=\"text\",\n",
    "    title=\"Audio Transcriber chatbot\",\n",
    "    description=\"Talk to your audio file\",\n",
    "    \n",
    ")\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    interface_test.launch()\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  },
  "orig_nbformat": 4,
  "polyglot_notebook": {
   "kernelInfo": {
    "defaultKernelName": "csharp",
    "items": [
     {
      "aliases": [],
      "name": "csharp"
     }
    ]
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
